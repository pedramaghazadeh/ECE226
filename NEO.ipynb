{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Embedding is treated as a zero-op.\n",
      "Warning: module Dropout is treated as a zero-op.\n",
      "Warning: module GPTNeoSelfAttention is treated as a zero-op.\n",
      "Warning: module GPTNeoAttention is treated as a zero-op.\n",
      "Warning: module NewGELUActivation is treated as a zero-op.\n",
      "Warning: module GPTNeoMLP is treated as a zero-op.\n",
      "Warning: module GPTNeoBlock is treated as a zero-op.\n",
      "Warning: module GPTNeoModel is treated as a zero-op.\n",
      "Warning: module GPTNeoForCausalLM is treated as a zero-op.\n",
      "Flops estimation was not finished successfully because of the following exception:\n",
      "<class 'RuntimeError'> : Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
      "=== FLOPs and Parameter Count ===\n",
      "MACs: None\n",
      "Params: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/ptflops/pytorch_engine.py\", line 68, in get_flops_pytorch\n",
      "    _ = flops_model(batch)\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1844, in _call_impl\n",
      "    return inner()\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1790, in inner\n",
      "    result = forward_call(*args, **kwargs)\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\", line 738, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\", line 581, in forward\n",
      "    inputs_embeds = self.wte(input_ids)\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/sparse.py\", line 190, in forward\n",
      "    return F.embedding(\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/functional.py\", line 2551, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Memory Usage (sorted by CPU memory consumption) ===\n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  aten::mul         0.80%       1.896ms         1.18%       2.787ms      28.731us      15.00 Mb      15.00 Mb            97  \n",
      "                  aten::add         0.89%       2.115ms         0.97%       2.286ms      18.894us       9.45 Mb       9.45 Mb           121  \n",
      "               aten::linear         0.59%       1.391ms        93.60%     221.399ms       1.527ms       9.40 Mb           0 b           145  \n",
      "                aten::addmm        63.27%     149.649ms        63.77%     150.840ms       2.095ms       5.62 Mb       5.62 Mb            72  \n",
      "               aten::matmul         0.53%       1.256ms        29.71%      70.288ms     580.889us       4.75 Mb           0 b           121  \n",
      "                   aten::mm        27.87%      65.924ms        27.88%      65.954ms     903.480us       3.77 Mb       3.77 Mb            73  \n",
      "                  aten::pow         0.23%     534.490us         0.23%     553.933us      23.081us       3.75 Mb       3.75 Mb            24  \n",
      "                 aten::tanh         0.21%     490.093us         0.21%     490.093us      20.421us       3.75 Mb       3.75 Mb            24  \n",
      "                aten::empty         0.22%     516.758us         0.22%     516.758us       2.328us       2.85 Mb       2.85 Mb           222  \n",
      "           aten::layer_norm         0.07%     170.539us         1.00%       2.375ms      48.474us       1.92 Mb           0 b            49  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 236.540ms\n",
      "\n",
      "\n",
      "=== Generated Text ===\n",
      "Give me a recipe for okonomiyaki.\n",
      "\n",
      "Okonomiyaki is a Japanese dish that is made with a combination of rice, soy sauce, and vinegar. It is a popular dish in Japan, and is often served with a variety of toppings.\n",
      "\n",
      "The dish is made by cooking rice in a wok, then adding soy sauce and vinegar to the rice. The rice is then cooked again, and the result is a delicious dish.\n",
      "\n",
      "The dish is a popular\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_quantized_model(model_name=\"EleutherAI/gpt-neo-1.3B\"):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.to(\"cpu\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_text(prompt, tokenizer, model, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {key: value.to(\"cpu\") for key, value in inputs.items()}\n",
    "    output_ids = model.generate(**inputs, max_length=max_length)\n",
    "    text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "def measure_flops(model, sequence_length=50):\n",
    "    \"\"\"\n",
    "    Measure FLOPs and parameter count using ptflops.\n",
    "    Note: For quantized models, reported FLOPs might not fully reflect low-precision ops.\n",
    "    \"\"\"\n",
    "    from ptflops import get_model_complexity_info\n",
    "\n",
    "    dummy_input_shape = (1, sequence_length)  \n",
    "    macs, params = get_model_complexity_info(\n",
    "        model, dummy_input_shape, as_strings=True,\n",
    "        print_per_layer_stat=True, verbose=True\n",
    "    )\n",
    "    print(\"=== FLOPs and Parameter Count ===\")\n",
    "    print(\"MACs:\", macs)\n",
    "    print(\"Params:\", params)\n",
    "\n",
    "def measure_memory(model, tokenizer, prompt=\"Test prompt\"):\n",
    "    \"\"\"\n",
    "    Measure memory usage during a forward pass using torch.profiler.\n",
    "    \"\"\"\n",
    "    import torch.profiler\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {key: value.to(\"cpu\") for key, value in inputs.items()}\n",
    "\n",
    "    with torch.profiler.profile(\n",
    "        activities=[torch.profiler.ProfilerActivity.CPU],\n",
    "        profile_memory=True,\n",
    "        record_shapes=True,\n",
    "    ) as prof:\n",
    "        model(**inputs)\n",
    "\n",
    "    print(\"=== Memory Usage (sorted by CPU memory consumption) ===\")\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))\n",
    "\n",
    "def main():\n",
    "    model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = load_quantized_model(model_name)\n",
    "    measure_flops(model, sequence_length=50)\n",
    "\n",
    "    measure_memory(model, tokenizer, prompt=\"Test prompt for memory profiling\")\n",
    "    \n",
    "    prompt = \"Give me a recipe for okonomiyaki\"\n",
    "    generated_text = generate_text(prompt, tokenizer, model)\n",
    "    print(\"\\n=== Generated Text ===\")\n",
    "    print(generated_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Embedding is treated as a zero-op.\n",
      "Warning: module Dropout is treated as a zero-op.\n",
      "Warning: module LinearPackedParams is treated as a zero-op.\n",
      "Warning: module Linear is treated as a zero-op.\n",
      "Warning: module GPTNeoSelfAttention is treated as a zero-op.\n",
      "Warning: module GPTNeoAttention is treated as a zero-op.\n",
      "Warning: module NewGELUActivation is treated as a zero-op.\n",
      "Warning: module GPTNeoMLP is treated as a zero-op.\n",
      "Warning: module GPTNeoBlock is treated as a zero-op.\n",
      "Warning: module GPTNeoModel is treated as a zero-op.\n",
      "Warning: module GPTNeoForCausalLM is treated as a zero-op.\n",
      "Flops estimation was not finished successfully because of the following exception:\n",
      "<class 'RuntimeError'> : Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
      "=== FLOPs and Parameter Count ===\n",
      "MACs: None\n",
      "Params: None\n",
      "=== Memory Usage (sorted by CPU memory consumption) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/ptflops/pytorch_engine.py\", line 68, in get_flops_pytorch\n",
      "    _ = flops_model(batch)\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1844, in _call_impl\n",
      "    return inner()\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1790, in inner\n",
      "    result = forward_call(*args, **kwargs)\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\", line 738, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\", line 581, in forward\n",
      "    inputs_embeds = self.wte(input_ids)\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/sparse.py\", line 190, in forward\n",
      "    return F.embedding(\n",
      "  File \"/home/dice/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/functional.py\", line 2551, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  aten::empty         1.27%     771.316us         1.27%     771.316us       1.506us      21.26 Mb      21.25 Mb           512  \n",
      "                    aten::mul         2.08%       1.259ms         2.71%       1.642ms      16.923us      15.00 Mb      15.00 Mb            97  \n",
      "             aten::empty_like         0.34%     206.366us         0.92%     559.202us       3.309us      10.33 Mb     200.00 Kb           169  \n",
      "                    aten::add         1.57%     950.328us         1.71%       1.033ms       8.535us       9.45 Mb       9.45 Mb           121  \n",
      "    quantized::linear_dynamic        85.00%      51.443ms        86.13%      52.122ms     359.462us       9.40 Mb      -9.20 Mb           145  \n",
      "                    aten::pow         0.68%     413.554us         0.70%     426.014us      17.751us       3.75 Mb       3.75 Mb            24  \n",
      "                   aten::tanh         0.53%     320.875us         0.53%     320.875us      13.370us       3.75 Mb       3.75 Mb            24  \n",
      "             aten::layer_norm         0.12%      75.508us         1.96%       1.188ms      24.252us       1.92 Mb           0 b            49  \n",
      "      aten::native_layer_norm         1.48%     894.988us         1.84%       1.113ms      22.712us       1.92 Mb          40 b            49  \n",
      "                 aten::matmul         0.55%     335.200us         2.86%       1.729ms      36.027us     997.50 Kb           0 b            48  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 60.519ms\n",
      "\n",
      "\n",
      "=== Generated Text ===\n",
      "Give me a recipe for okonomiyaki.\n",
      "\n",
      "Okonomiyaki is a Japanese term for “good” or “healthy”. It is a dish that is made with rice, and is often served with a side of vegetables. It is a popular dish in Japan, and is often served at Japanese restaurants.\n",
      "\n",
      "The word “okonomiyaki” is derived from the Japanese word for “good” or “healthy”.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_quantized_model(model_name=\"EleutherAI/gpt-neo-1.3B\"):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.to(\"cpu\")\n",
    "    quantized_model = torch.quantization.quantize_dynamic(\n",
    "        model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "    )\n",
    "    return quantized_model\n",
    "\n",
    "def generate_text(prompt, tokenizer, model, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {key: value.to(\"cpu\") for key, value in inputs.items()}\n",
    "    output_ids = model.generate(**inputs, max_length=max_length)\n",
    "    text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "def measure_flops(model, sequence_length=50):\n",
    "    \"\"\"\n",
    "    Measure FLOPs and parameter count using ptflops.\n",
    "    Note: For quantized models, reported FLOPs might not fully reflect low-precision ops.\n",
    "    \"\"\"\n",
    "    from ptflops import get_model_complexity_info\n",
    "\n",
    "    dummy_input_shape = (1, sequence_length)  \n",
    "    macs, params = get_model_complexity_info(\n",
    "        model, dummy_input_shape, as_strings=True,\n",
    "        print_per_layer_stat=True, verbose=True\n",
    "    )\n",
    "    print(\"=== FLOPs and Parameter Count ===\")\n",
    "    print(\"MACs:\", macs)\n",
    "    print(\"Params:\", params)\n",
    "\n",
    "def measure_memory(model, tokenizer, prompt=\"Test prompt\"):\n",
    "    \"\"\"\n",
    "    Measure memory usage during a forward pass using torch.profiler.\n",
    "    \"\"\"\n",
    "    import torch.profiler\n",
    "\n",
    "    # Create a dummy input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {key: value.to(\"cpu\") for key, value in inputs.items()}\n",
    "\n",
    "    with torch.profiler.profile(\n",
    "        activities=[torch.profiler.ProfilerActivity.CPU],\n",
    "        profile_memory=True,\n",
    "        record_shapes=True,\n",
    "    ) as prof:\n",
    "        model(**inputs)\n",
    "\n",
    "    print(\"=== Memory Usage (sorted by CPU memory consumption) ===\")\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))\n",
    "\n",
    "def main():\n",
    "    model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = load_quantized_model(model_name)\n",
    "    measure_flops(model, sequence_length=50)\n",
    "    measure_memory(model, tokenizer, prompt=\"Test prompt for memory profiling\")\n",
    "    prompt = \"Give me a recipe for okonomiyaki\"\n",
    "    generated_text = generate_text(prompt, tokenizer, model)\n",
    "    print(\"\\n=== Generated Text ===\")\n",
    "    print(generated_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
