{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pedramaghazadeh/miniconda3/envs/echollm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import resource\n",
    "import transformers\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_flops(model, sequence_length=50):\n",
    "    \"\"\"\n",
    "    Measure FLOPs and parameter count using ptflops.\n",
    "    Note: For quantized models, reported FLOPs might not fully reflect low-precision ops.\n",
    "    \"\"\"\n",
    "    from ptflops import get_model_complexity_info\n",
    "\n",
    "    dummy_input_shape = (1, sequence_length)  \n",
    "    macs, params = get_model_complexity_info(\n",
    "        model, dummy_input_shape, as_strings=True,\n",
    "        print_per_layer_stat=True, verbose=True\n",
    "    )\n",
    "    print(\"=== FLOPs and Parameter Count ===\")\n",
    "    print(\"MACs:\", macs)\n",
    "    print(\"Params:\", params)\n",
    "\n",
    "\n",
    "def measure_memory(model, tokenizer, prompt=\"Test prompt\"):\n",
    "    \"\"\"\n",
    "    Measure memory usage during a forward pass using torch.profiler.\n",
    "    \"\"\"\n",
    "    import torch.profiler\n",
    "\n",
    "    # Create a dummy input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {key: value.to(\"cpu\") for key, value in inputs.items()}\n",
    "\n",
    "    with torch.profiler.profile(\n",
    "        activities=[torch.profiler.ProfilerActivity.CPU],\n",
    "        profile_memory=True,\n",
    "        record_shapes=True,\n",
    "    ) as prof:\n",
    "        model(**inputs)\n",
    "\n",
    "    print(\"=== Memory Usage (sorted by CPU memory consumption) ===\")\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))\n",
    "\n",
    "\n",
    "def nshot_chats(nshot_data: list, n: int, question: str) -> dict:\n",
    "\n",
    "    def question_prompt(s):\n",
    "        return f'Question: {s}'\n",
    "\n",
    "    def answer_prompt(s):\n",
    "        return f'Answer: {s}'\n",
    "\n",
    "    chats = []\n",
    "\n",
    "    random.seed(42)\n",
    "    for qna in random.sample(nshot_data, n):\n",
    "        chats.append(\n",
    "            {\"role\": \"user\", \"content\": question_prompt(qna[\"question\"])})\n",
    "        chats.append(\n",
    "            {\"role\": \"assistant\", \"content\": answer_prompt(qna[\"answer\"])})\n",
    "\n",
    "    chats.append({\"role\": \"user\", \"content\": question_prompt(question)+\" Let's think step by step. At the end, you MUST finish the sentence with '####' followed by the answer as an integer.\"})\n",
    "\n",
    "    return chats\n",
    "\n",
    "\n",
    "def extract_ans_from_response(answer: str, eos=None):\n",
    "    if eos:\n",
    "        answer = answer.split(eos)[0].strip()\n",
    "\n",
    "    answer = answer.split('####')[-1].strip()\n",
    "\n",
    "    for remove_char in [',', '$', '%', 'g']:\n",
    "        answer = answer.replace(remove_char, '')\n",
    "\n",
    "    try:\n",
    "        return int(answer)\n",
    "    except ValueError:\n",
    "        return answer\n",
    "\n",
    "\n",
    "def preprocess(example):\n",
    "    options = ' '.join(example['options'])\n",
    "    prompt = f\"Question: {example['Problem']}\\nOptions: {options}\\nAnswer:\"\n",
    "    return {\"question\": prompt, \"answer\": example['correct']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(\n",
    "    # model=\"EleutherAI/gpt-neo-2.7B\",\n",
    "    model=\"facebook/opt-2.7B\",\n",
    "    token=\"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\",\n",
    "    device=\"cpu\",\n",
    "    quantize=False,\n",
    "    eval=[\"gsm8k\"],\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(args.model, token=args.token).to(args.device)\n",
    "if args.quantize:\n",
    "    model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model, token=args.token, return_dict=True)\n",
    "if tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = \"\"\"\n",
    "    {% for message in messages %}\n",
    "    {% if message['role'] == 'user' %}\n",
    "    User: {{ message['content'] }}\n",
    "    {% elif message['role'] == 'assistant' %}\n",
    "    Assistant: {{ message['content'] }}\n",
    "    {% endif %}\n",
    "    {% endfor %}\n",
    "    Assistant:\n",
    "    \"\"\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "generator = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=args.device, max_new_tokens=512)\n",
    "\n",
    "def get_response(chats):\n",
    "    # chats = tokenizer.apply_chat_template(chats, tokenize=False, add_generation_prompt=True)\n",
    "    gen_text = generator(chats)  # First return sequence\n",
    "    print(gen_text)\n",
    "    print(gen_text[0][\"generated_text\"][-1][\"content\"])\n",
    "    return gen_text[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "\n",
    "def extract_answer(answer: str, eos=None):\n",
    "    if eos:\n",
    "        answer = answer.split(eos)[0].strip()\n",
    "\n",
    "    answer = answer.split('####')[-1].strip()\n",
    "    # Removing possible non-numeric characters generated by the LLM\n",
    "    for remove_char in [',', '$', '%', 'g']:\n",
    "        answer = answer.replace(remove_char, '')\n",
    "\n",
    "    try:\n",
    "        return int(answer)\n",
    "    except ValueError:\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing whether the model works correctly or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'generated_text': 'Question: What is the capital of France?\\nParis.'}]]\n"
     ]
    }
   ],
   "source": [
    "print(generator([\"Question: What is the capital of France?\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    User: Question, what is the capital of France?\n",
      "    Assistant:\n",
      "    \n",
      "[{'generated_text': '\\n    User: Question, what is the capital of France?\\n    Assistant:\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    '}]\n"
     ]
    }
   ],
   "source": [
    "message = [{\"role\": \"system\", \"content\": \"You're a helpful AI chatbot serving as an assistant to user.\"}, {\"role\": \"user\", \"content\": \"Question, what is the capital of France?\"}]\n",
    "print(tokenizer.apply_chat_template(message, tokenize=False))\n",
    "print(generator(tokenizer.apply_chat_template(message, tokenize=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'generated_text': 'User: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What is the capital of France?\\n Assistant:  Paris.\\nUser: Question: What'}]]\n"
     ]
    }
   ],
   "source": [
    "print(generator([\"User: Question: What is the capital of France?\\n Assistant: \"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m((\u001b[38;5;28mlen\u001b[39m(test_data)))):\n\u001b[1;32m     11\u001b[0m     messages \u001b[38;5;241m=\u001b[39m nshot_chats(nshot_data\u001b[38;5;241m=\u001b[39mtrain_data, n\u001b[38;5;241m=\u001b[39mNUMBER_SHOT, question\u001b[38;5;241m=\u001b[39mtest_data[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 12\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m extract_answer(response) \u001b[38;5;241m==\u001b[39m extract_answer(test_data[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     15\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m, in \u001b[0;36mget_response\u001b[0;34m(chats)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_response\u001b[39m(chats):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# chats = tokenizer.apply_chat_template(chats, tokenize=False, add_generation_prompt=True)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     gen_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchats\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# First return sequence\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(gen_text)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(gen_text[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/echollm/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:280\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 280\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m         chats \u001b[38;5;241m=\u001b[39m (Chat(chat) \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m text_inputs)  \u001b[38;5;66;03m# 🐈 🐈 🐈\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/echollm/lib/python3.12/site-packages/transformers/pipelines/base.py:1368\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1361\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1362\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         )\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/echollm/lib/python3.12/site-packages/transformers/pipelines/base.py:1374\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1374\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1376\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/echollm/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:316\u001b[0m, in \u001b[0;36mTextGenerationPipeline.preprocess\u001b[0;34m(self, prompt_text, prefix, handle_long_generation, add_special_tokens, truncation, padding, max_length, continue_final_message, **generate_kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_final_message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m         continue_final_message \u001b[38;5;241m=\u001b[39m prompt_text\u001b[38;5;241m.\u001b[39mmessages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 316\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontinue_final_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontinue_final_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontinue_final_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(prefix \u001b[38;5;241m+\u001b[39m prompt_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/echollm/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1629\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1627\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1629\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*generation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_template):\n\u001b[1;32m   1632\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1633\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[38;5;124meneration \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}` keyword.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1634\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/echollm/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1805\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   1803\u001b[0m         chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_template\n\u001b[1;32m   1804\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1805\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1806\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1807\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument was passed! For information about writing templates and setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1808\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1809\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1810\u001b[0m         )\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "# Measure FLOPs and parameter count\n",
    "# print(f\"Model {args.model} has FLOPs of {measure_flops(model)}\")\n",
    "\n",
    "if \"gsm8k\" in args.eval:\n",
    "    NUMBER_SHOT = 3\n",
    "    correct = 0\n",
    "    train_data = list(load_dataset(\"gsm8k\", \"main\", split=\"train[:20]\"))\n",
    "    test_data = list(load_dataset(\"gsm8k\", \"main\", split=\"test[:1]\"))\n",
    "\n",
    "    for i in tqdm(range((len(test_data)))):\n",
    "        messages = nshot_chats(nshot_data=train_data, n=NUMBER_SHOT, question=test_data[i]['question'])\n",
    "        response = get_response(messages)\n",
    "\n",
    "        if extract_answer(response) == extract_answer(test_data[i]['answer']):\n",
    "            correct += 1\n",
    "    print(f\"Accuracy of {args.model}: {correct/len(test_data)}\")\n",
    "\n",
    "if \"mathqa\" in args.eval:\n",
    "    mathqa = load_dataset(\"mathqa\", split=\"train[:5]\")\n",
    "\n",
    "    mathqa = mathqa.map(preprocess)\n",
    "    for i in range(5):\n",
    "        messages = nshot_chats(nshot_data=mathqa, n=0, question=mathqa[i]['problem'])\n",
    "        response = get_response(messages)\n",
    "        print(f\"Question: {mathqa[i]['problem']}\")\n",
    "        print(f\"Answer: {mathqa[i]['answer']}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Extracted Answer: {extract_answer(response)}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "echollm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
