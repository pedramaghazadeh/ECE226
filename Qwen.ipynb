{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "hf_token = userdata.get(\"huggingface\")"
      ],
      "metadata": {
        "id": "ks6fpMQ44GX6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ptflops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Leg7Rdc4lFe",
        "outputId": "69ae270f-998f-4680-fc9e-5b92dc839dc2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ptflops in /usr/local/lib/python3.11/dist-packages (0.7.4)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from ptflops) (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20wSHUtb3PTi",
        "outputId": "4536493f-1ebe-4753-d670-623795bac3ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: module Embedding is treated as a zero-op.\n",
            "Warning: module Qwen2Attention is treated as a zero-op.\n",
            "Warning: module SiLU is treated as a zero-op.\n",
            "Warning: module Qwen2MLP is treated as a zero-op.\n",
            "Warning: module Qwen2RMSNorm is treated as a zero-op.\n",
            "Warning: module Qwen2DecoderLayer is treated as a zero-op.\n",
            "Warning: module Qwen2RotaryEmbedding is treated as a zero-op.\n",
            "Warning: module Qwen2Model is treated as a zero-op.\n",
            "Warning: module Qwen2ForCausalLM is treated as a zero-op.\n",
            "Flops estimation was not finished successfully because of the following exception:\n",
            "<class 'RuntimeError'> : Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
            "=== FLOPs and Parameter Count ===\n",
            "MACs: None\n",
            "Params: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ptflops/pytorch_engine.py\", line 68, in get_flops_pytorch\n",
            "    _ = flops_model(batch)\n",
            "        ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1844, in _call_impl\n",
            "    return inner()\n",
            "           ^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1790, in inner\n",
            "    result = forward_call(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 819, in forward\n",
            "    outputs = self.model(\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 533, in forward\n",
            "    inputs_embeds = self.embed_tokens(input_ids)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\", line 190, in forward\n",
            "    return F.embedding(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2551, in embedding\n",
            "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Memory Usage (sorted by CPU memory consumption) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Both `max_new_tokens` (=2048) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                         aten::linear         0.23%       3.854ms        86.72%        1.476s       7.495ms      15.20 Mb           0 b           197  \n",
            "                                         aten::matmul         0.42%       7.141ms        81.16%        1.382s      12.120ms      14.11 Mb           0 b           114  \n",
            "                                             aten::mm        79.91%        1.361s        80.04%        1.363s      12.059ms      14.11 Mb      14.11 Mb           113  \n",
            "                                            aten::mul         0.89%      15.087ms         0.89%      15.139ms      59.135us      10.04 Mb      10.04 Mb           256  \n",
            "                                           aten::silu         1.00%      17.091ms         1.00%      17.091ms     610.397us       4.79 Mb       4.79 Mb            28  \n",
            "                                     aten::empty_like         0.04%     761.105us         0.14%       2.395ms      17.107us       4.10 Mb           0 b           140  \n",
            "                                          aten::empty         0.09%       1.598ms         0.09%       1.598ms       8.070us       3.36 Mb       3.36 Mb           198  \n",
            "                                          aten::clone         0.22%       3.804ms         0.53%       9.074ms      81.014us       3.28 Mb           0 b           112  \n",
            "                                            aten::add         0.80%      13.573ms         0.84%      14.331ms      84.801us       2.60 Mb       2.60 Mb           169  \n",
            "                                            aten::pow         0.27%       4.570ms         0.27%       4.677ms      82.051us       1.67 Mb       1.67 Mb            57  \n",
            "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 1.702s\n",
            "\n",
            "\n",
            "=== Generated Text ===\n",
            "Give me a recipe for okonomiyaki. I'm sorry, as an AI language model, I don't have the ability to cook or create recipes. However, I can provide you with some general information on how to make okonomiyaki.\n",
            "\n",
            "Okonomiyaki is a traditional Japanese dish made with rice, meat, and vegetables. Here's a simple recipe to get you started:\n",
            "\n",
            "Ingredients:\n",
            "- 1 cup of rice\n",
            "- 1/2 cup of water\n",
            "- 1/2 cup of oil\n",
            "- 1/2 cup of vinegar\n",
            "- 1/2 cup of salt\n",
            "- 1/2 cup of pepper\n",
            "- 1/2 cup of ground beef\n",
            "- 1/2 cup of sliced vegetables (such as carrots, potatoes, or mushrooms)\n",
            "\n",
            "Instructions:\n",
            "1. Preheat the oven to 350°F (175°C).\n",
            "2. In a large bowl, mix together the rice, water, oil, vinegar, salt, pepper, ground beef, and sliced vegetables.\n",
            "3. Pour the mixture into a shallow baking dish and bake for 15-20 minutes, or until the rice is golden brown and the vegetables are soft.\n",
            "4. Remove the baking dish from the oven and let it cool for a few minutes before serving.\n",
            "\n",
            "Note: You can adjust the amount of meat and vegetables in the recipe to suit your taste preferences. You can also add more or less water and oil to achieve the desired consistency.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "def load_quantized_model(model_name=\"EleutherAI/gpt-neo-1.3B\"):\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model.to(\"cpu\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def generate_text(prompt, tokenizer, model, max_length=100):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {key: value.to(\"cpu\") for key, value in inputs.items()}\n",
        "    output_ids = model.generate(**inputs, max_length=max_length)\n",
        "    text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return text\n",
        "\n",
        "def measure_flops(model, sequence_length=50):\n",
        "    \"\"\"\n",
        "    Measure FLOPs and parameter count using ptflops.\n",
        "    Note: For quantized models, reported FLOPs might not fully reflect low-precision ops.\n",
        "    \"\"\"\n",
        "    from ptflops import get_model_complexity_info\n",
        "\n",
        "    dummy_input_shape = (1, sequence_length)\n",
        "    macs, params = get_model_complexity_info(\n",
        "        model, dummy_input_shape, as_strings=True,\n",
        "        print_per_layer_stat=True, verbose=True\n",
        "    )\n",
        "    print(\"=== FLOPs and Parameter Count ===\")\n",
        "    print(\"MACs:\", macs)\n",
        "    print(\"Params:\", params)\n",
        "\n",
        "def measure_memory(model, tokenizer, prompt=\"Test prompt\"):\n",
        "    \"\"\"\n",
        "    Measure memory usage during a forward pass using torch.profiler.\n",
        "    \"\"\"\n",
        "    import torch.profiler\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {key: value.to(\"cpu\") for key, value in inputs.items()}\n",
        "\n",
        "    with torch.profiler.profile(\n",
        "        activities=[torch.profiler.ProfilerActivity.CPU],\n",
        "        profile_memory=True,\n",
        "        record_shapes=True,\n",
        "    ) as prof:\n",
        "        model(**inputs)\n",
        "\n",
        "    print(\"=== Memory Usage (sorted by CPU memory consumption) ===\")\n",
        "    print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))\n",
        "\n",
        "def main():\n",
        "    model_name = \"Qwen/Qwen2.5-Math-1.5B\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = load_quantized_model(model_name)\n",
        "    measure_flops(model, sequence_length=50)\n",
        "\n",
        "    measure_memory(model, tokenizer, prompt=\"Test prompt for memory profiling\")\n",
        "\n",
        "    prompt = \"Give me a recipe for okonomiyaki\"\n",
        "    generated_text = generate_text(prompt, tokenizer, model)\n",
        "    print(\"\\n=== Generated Text ===\")\n",
        "    print(generated_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B9fg7aX3PTj",
        "outputId": "08b7ad40-a7cf-4807-f6d0-435dd1d86851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "def load_quantized_model(model_name=\"EleutherAI/gpt-neo-1.3B\"):\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model.to(\"cpu\")\n",
        "    quantized_model = torch.quantization.quantize_dynamic(\n",
        "        model, {torch.nn.Linear}, dtype=torch.qint8\n",
        "    )\n",
        "    return quantized_model\n",
        "\n",
        "def generate_text(prompt, tokenizer, model, max_length=100):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {key: value.to(\"cpu\") for key, value in inputs.items()}\n",
        "    output_ids = model.generate(**inputs, max_length=max_length)\n",
        "    text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return text\n",
        "\n",
        "def measure_flops(model, sequence_length=50):\n",
        "    \"\"\"\n",
        "    Measure FLOPs and parameter count using ptflops.\n",
        "    Note: For quantized models, reported FLOPs might not fully reflect low-precision ops.\n",
        "    \"\"\"\n",
        "    from ptflops import get_model_complexity_info\n",
        "\n",
        "    dummy_input_shape = (1, sequence_length)\n",
        "    macs, params = get_model_complexity_info(\n",
        "        model, dummy_input_shape, as_strings=True,\n",
        "        print_per_layer_stat=True, verbose=True\n",
        "    )\n",
        "    print(\"=== FLOPs and Parameter Count ===\")\n",
        "    print(\"MACs:\", macs)\n",
        "    print(\"Params:\", params)\n",
        "\n",
        "def measure_memory(model, tokenizer, prompt=\"Test prompt\"):\n",
        "    \"\"\"\n",
        "    Measure memory usage during a forward pass using torch.profiler.\n",
        "    \"\"\"\n",
        "    import torch.profiler\n",
        "\n",
        "    # Create a dummy input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {key: value.to(\"cpu\") for key, value in inputs.items()}\n",
        "\n",
        "    with torch.profiler.profile(\n",
        "        activities=[torch.profiler.ProfilerActivity.CPU],\n",
        "        profile_memory=True,\n",
        "        record_shapes=True,\n",
        "    ) as prof:\n",
        "        model(**inputs)\n",
        "\n",
        "    print(\"=== Memory Usage (sorted by CPU memory consumption) ===\")\n",
        "    print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))\n",
        "\n",
        "def main():\n",
        "    model_name = \"Qwen/Qwen2.5-Math-1.5B\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = load_quantized_model(model_name)\n",
        "    measure_flops(model, sequence_length=50)\n",
        "    measure_memory(model, tokenizer, prompt=\"Test prompt for memory profiling\")\n",
        "    prompt = \"Give me a recipe for okonomiyaki\"\n",
        "    generated_text = generate_text(prompt, tokenizer, model)\n",
        "    print(\"\\n=== Generated Text ===\")\n",
        "    print(generated_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With GSM1K"
      ],
      "metadata": {
        "id": "We6Y20Bz-9dH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries for HuggingFace and ptflops\n",
        "!pip install transformers torch ptflops\n",
        "\n",
        "import torch\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load the quantized model\n",
        "def load_quantized_model(model_name=\"Qwen/Qwen2.5-Math-1.5B\"):\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model.to(\"cpu\")\n",
        "    return model\n",
        "\n",
        "# Generate text from the model\n",
        "def generate_text(prompt, tokenizer, model, max_length=100):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {key: value.to(\"cpu\") for key, value in inputs.items()}\n",
        "    output_ids = model.generate(**inputs, max_length=max_length)\n",
        "    text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return text\n",
        "\n",
        "# Measure FLOPs and parameters\n",
        "def measure_flops(model, sequence_length=50):\n",
        "    from ptflops import get_model_complexity_info\n",
        "\n",
        "    dummy_input_shape = (1, sequence_length)\n",
        "    macs, params = get_model_complexity_info(\n",
        "        model, dummy_input_shape, as_strings=True,\n",
        "        print_per_layer_stat=True, verbose=True\n",
        "    )\n",
        "    print(\"=== FLOPs and Parameter Count ===\")\n",
        "    print(\"MACs:\", macs)\n",
        "    print(\"Params:\", params)\n",
        "\n",
        "# Measure memory usage during a forward pass\n",
        "def measure_memory(model, tokenizer, prompt=\"Test prompt\"):\n",
        "    import torch.profiler\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {key: value.to(\"cpu\") for key, value in inputs.items()}\n",
        "\n",
        "    with torch.profiler.profile(\n",
        "        activities=[torch.profiler.ProfilerActivity.CPU],\n",
        "        profile_memory=True,\n",
        "        record_shapes=True,\n",
        "    ) as prof:\n",
        "        model(**inputs)\n",
        "\n",
        "    print(\"=== Memory Usage (sorted by CPU memory consumption) ===\")\n",
        "    print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))\n",
        "\n",
        "# Load GSM1K dataset (from a URL or local file)\n",
        "def load_gsm1k_dataset(dataset_url=\"https://raw.githubusercontent.com/scaleapi/gsm1k_eval/main/data/gsm1k_test.json\"):\n",
        "    import requests\n",
        "\n",
        "    response = requests.get(dataset_url)\n",
        "    data = response.json()\n",
        "\n",
        "    return data\n",
        "\n",
        "# Main function to run the entire process\n",
        "def main():\n",
        "    # Model and tokenizer setup\n",
        "    model_name = \"Qwen/Qwen2.5-Math-1.5B\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = load_quantized_model(model_name)\n",
        "\n",
        "    # Load the GSM1K dataset\n",
        "    gsm1k_data = load_gsm1k_dataset()\n",
        "\n",
        "    # Measure model complexity\n",
        "    measure_flops(model, sequence_length=50)\n",
        "\n",
        "    # Measure memory usage on a sample question from GSM1K dataset\n",
        "    sample_question = gsm1k_data[0][\"question\"]  # Get the first math problem in the dataset\n",
        "    print(f\"Sample Question: {sample_question}\")\n",
        "    measure_memory(model, tokenizer, prompt=sample_question)\n",
        "\n",
        "    # Generate text for a batch of questions from GSM1K\n",
        "    for item in gsm1k_data[:5]:  # Just process the first 5 items for demo purposes\n",
        "        question = item[\"question\"]\n",
        "        print(f\"\\nGenerating answer for: {question}\")\n",
        "        generated_answer = generate_text(question, tokenizer, model)\n",
        "        print(\"Generated Answer:\", generated_answer)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "JX8bWr98946L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}